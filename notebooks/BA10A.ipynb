{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5fba88",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Chapter BA10A: Probability of a Hidden Path\"\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "    toc: true\n",
    "jupyter: python3\n",
    "---\n",
    "\n",
    "## Problem Statement and Biological Context\n",
    "\n",
    "**Given:** A hidden path π followed by the states States and transition matrix Transition of an HMM (Σ, States, Transition, Emission).\n",
    "\n",
    "**Return:** The probability of this path, $Pr(\\pi)$. You may assume that initial probabilities are equal.\n",
    "\n",
    "\n",
    "**Sample Dataset:**\n",
    "```\n",
    "AABBBAABABAAAABBBBAABBABABBBAABBAAAABABAABBABABBAB\n",
    "--------\n",
    "A   B\n",
    "--------\n",
    "    A   B\n",
    "A   0.194   0.806\n",
    "B   0.273   0.727\n",
    "```\n",
    "\n",
    "**Sample Output:**\n",
    "```\n",
    "5.01732865318e-19\n",
    "```\n",
    "\n",
    "This problem introduces us to Hidden Markov Models (HMMs), one of the most fundamental statistical frameworks in bioinformatics[1][2]. HMMs are probabilistic models that describe sequences where the underlying process generating the observations is hidden from direct view[2][8]. In biological contexts, HMMs are extensively used for gene finding, protein structure prediction, sequence alignment, and identifying functional domains in proteins[8][29].\n",
    "\n",
    "The hidden path probability calculation forms the foundation for more complex HMM algorithms like the Viterbi algorithm for decoding and the Forward-Backward algorithm for parameter estimation[5][6]. Understanding this computation is essential because it represents the core mathematical operation that quantifies how likely a particular sequence of hidden states is given the model parameters.\n",
    "\n",
    "In genomics, this could model scenarios such as CG-island detection (where hidden states represent \"inside\" vs \"outside\" CG-rich regions), gene structure annotation (exons vs introns), or chromatin state segmentation (active vs inactive transcription regions)[1][42]. The transition probabilities capture the biological tendency for these states to persist or change over genomic coordinates.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Hidden Markov Model Definition\n",
    "\n",
    "A Hidden Markov Model is formally defined as a tuple $$ M = (Q, \\Sigma, A, E, \\pi) $$ where[2][8]:\n",
    "\n",
    "- $( Q = \\{q_1, q_2, \\ldots, q_N\\} )$ is the set of hidden states\n",
    "- $( \\Sigma )$ is the observation alphabet  \n",
    "- $( A )$ is the $( N \\times N )$ state transition matrix\n",
    "- $( E )$ is the $( N \\times |\\Sigma| )$ emission matrix\n",
    "- $( \\pi )$ is the initial state probability distribution\n",
    "\n",
    "For this problem, we focus on the transition matrix $( A )$ where $$ A_{ij} = P(q_{t+1} = j \\mid q_t = i) $$ represents the probability of transitioning from state $( i )$ to state $( j )$.\n",
    "\n",
    "### Hidden Path Probability Calculation\n",
    "\n",
    "Given a hidden path $( \\pi = \\pi_1\\pi_2\\ldots\\pi_L )$ of length $( L )$, the probability of this path is computed using the Markov property[2][5]:\n",
    "\n",
    "$$P(\\pi) = P(\\pi_1) \\prod_{t=2}^{L} P(\\pi_t | \\pi_{t-1})$$\n",
    "\n",
    "Under the equal initial probability assumption, $P(\\pi_1) = \\frac{1}{|Q|}$, so:\n",
    "\n",
    "$$P(\\pi) = \\frac{1}{|Q|} \\prod_{t=2}^{L} A_{\\pi_{t-1}, \\pi_t}$$\n",
    "\n",
    "### Numerical Stability Considerations\n",
    "\n",
    "For long sequences, the probability product can become extremely small, leading to numerical underflow[40][43]. The standard approach is to work in log space:\n",
    "\n",
    "$$\\log P(\\pi) = \\log\\left(\\frac{1}{|Q|}\\right) + \\sum_{t=2}^{L} \\log A_{\\pi_{t-1},\\pi_t}$$\n",
    "\n",
    "This transforms the product into a sum, making computations numerically stable and more efficient[43][45].\n",
    "\n",
    "### Complexity Analysis\n",
    "\n",
    "The time complexity is $( O(L) )$ where $( L )$ is the path length, since we perform one transition probability lookup per position. Space complexity is $( O(1) )$ for the computation itself, plus $( O(N^2) )$ to store the transition matrix where $( N )$ is the number of states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e84ad",
   "metadata": {},
   "source": [
    "## Reference Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05a7df10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "from typing import List, Dict, Tuple\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed4e1da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
